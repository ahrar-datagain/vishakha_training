{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tensor Flow\n\nTensorFlow is a powerful open-source machine-learning framework developed by Google, that empowers developers to construct and train ML model.\n\n**TensorFlow** is basically a software library for numerical computation using **data flow graphs** where:\n\n*   **nodes** in the graph represent mathematical operations.\n    \n*   **edges** in the graph represent the multidimensional data arrays (called **tensors**) communicated between them. (Please note that **tensor** is the central unit of data in TensorFlow).\n\n![](https://media.geeksforgeeks.org/wp-content/uploads/graph1.png)\n\nHere, add = Node, a and b are input tensors and c is resultant tensor.\n\n### Tensors\nTensors are the primary data sructure in Tensorflow.They are multi-dimensional arrays. we can create them by **tf.constants** or **tf.variables.**\n\n#### Tensor Operations\n**1. Addition :**\n use tf.add()","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy\n\n# Ensure eager execution is enabled\ntf.config.run_functions_eagerly(True)\n\n# Creating tensors\nnode1 = tf.constant(3, dtype=tf.int32)\nnode2 = tf.constant(5, dtype=tf.int32)\n\n# Performing addition\nnode3 = tf.add(node1, node2)\n\n# Evaluating and printing the result\nprint(\"Sum of node1 and node2 is:\", node3.numpy())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T12:14:09.268427Z","iopub.execute_input":"2024-09-02T12:14:09.268931Z","iopub.status.idle":"2024-09-02T12:14:25.292243Z","shell.execute_reply.started":"2024-09-02T12:14:09.268882Z","shell.execute_reply":"2024-09-02T12:14:25.290763Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Sum of node1 and node2 is: 8\n","output_type":"stream"}]},{"cell_type":"markdown","source":"TensorFlow 2.x, by default, runs in eager execution mode, which means operations are executed immediately as they are defined. However, when using tf.compat.v1.Session(), it attempts to use the graph-based execution model from TensorFlow 1.x, and that's why we have used disable_eager_execution here.\n\nNode 1 and node 2 are constant type nodes, and df_type defines autput value.\nNode3 is of tf.add type. It takes two tensors as input and returns their sum as output tensor.","metadata":{}},{"cell_type":"markdown","source":"**Variables**\n-------------\n\nTensorFlow has **Variable** nodes too which can hold variable data. They are mainly used to hold and update parameters of a training model. Variables are **in-memory buffers** containing tensors. They must be explicitly initialized and can be saved to disk during and after training. You can later restore saved values to exercise or analyze the model. An important difference to note between a **constant** and **Variable** is:\n\n> A constant’s value is stored in the graph and its value is replicated wherever the graph is loaded. A variable is stored separately, and may live on a parameter server.\n\n**Note:** There is no need for tf.Session() in 2.x, Eager execution allows operations to be evaluated immediately as they are defined, eliminating the need for sessions and graphs as used in TensorFlow 1.x.","metadata":{"execution":{"iopub.status.busy":"2024-09-02T10:19:51.718336Z","iopub.execute_input":"2024-09-02T10:19:51.719619Z","iopub.status.idle":"2024-09-02T10:19:51.738419Z","shell.execute_reply.started":"2024-09-02T10:19:51.719561Z","shell.execute_reply":"2024-09-02T10:19:51.736628Z"}}},{"cell_type":"code","source":"import tensorflow as tf\n\n# create a variable\nnode = tf.Variable(tf.zeros([2, 2]))\n\n# TensorFlow 2.x automatically initializes variables\n# You can directly evaluate the variable\n#To assign a new value to a variable node, we can use assign method like this:\nnode = node.assign(node + tf.ones([2,2]))\n\nprint(\"Evaluating node:\\n\", node.numpy())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T12:14:25.294768Z","iopub.execute_input":"2024-09-02T12:14:25.295427Z","iopub.status.idle":"2024-09-02T12:14:25.325601Z","shell.execute_reply.started":"2024-09-02T12:14:25.295384Z","shell.execute_reply":"2024-09-02T12:14:25.324134Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Evaluating node:\n [[1. 1.]\n [1. 1.]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"##### Matrix Multiplication\n![](https://media.geeksforgeeks.org/wp-content/uploads/placeholder2.png)","metadata":{}},{"cell_type":"code","source":"\n# Define the input tensors\na = tf.constant([[1], [2], [3]], dtype=tf.int32)  # Example input\nb = tf.constant([[4, 5, 6]], dtype=tf.int32)      # Example input\n\n# Perform matrix multiplication\nc = tf.matmul(a, b)\n\n# Print the result\nprint(\"Result of matrix multiplication:\\n\", c.numpy())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T12:14:25.327501Z","iopub.execute_input":"2024-09-02T12:14:25.328333Z","iopub.status.idle":"2024-09-02T12:14:25.343913Z","shell.execute_reply.started":"2024-09-02T12:14:25.328276Z","shell.execute_reply":"2024-09-02T12:14:25.342274Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Result of matrix multiplication:\n [[ 4  5  6]\n [ 8 10 12]\n [12 15 18]]\n","output_type":"stream"}]},{"cell_type":"code","source":"# all tensor opertaions\ntensor_a = tf.constant([1, 2, 3, 4], dtype=tf.float32)\ntensor_b = tf.Variable([5, 6, 7, 8], dtype=tf.float32)\n\ntensor_sum = tf.add(tensor_a, tensor_b)\ntensor_prod = tf.multiply(tensor_a, tensor_b)\ntensor_reshaped = tf.reshape(tensor_a, [2,2])\nprint(\"Eager Execution - Sum:\", tensor_sum.numpy(), \"\\n\",tensor_prod.numpy(), \"\\n\", tensor_reshaped.numpy())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T12:14:25.347061Z","iopub.execute_input":"2024-09-02T12:14:25.347520Z","iopub.status.idle":"2024-09-02T12:14:25.372124Z","shell.execute_reply.started":"2024-09-02T12:14:25.347476Z","shell.execute_reply":"2024-09-02T12:14:25.370687Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Eager Execution - Sum: [ 6.  8. 10. 12.] \n [ 5. 12. 21. 32.] \n [[1. 2.]\n [3. 4.]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Variables**\n\nVariables are mutable and can be modified after creation.","metadata":{}},{"cell_type":"code","source":"variable_tensor = tf.Variable([1, 2, 3])\nvariable_tensor.assign([4, 5, 6])  # Update the value\nprint(\"Updated Variable:\", variable_tensor.numpy())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T12:14:25.374201Z","iopub.execute_input":"2024-09-02T12:14:25.374761Z","iopub.status.idle":"2024-09-02T12:14:25.392916Z","shell.execute_reply.started":"2024-09-02T12:14:25.374704Z","shell.execute_reply":"2024-09-02T12:14:25.390872Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Updated Variable: [4 5 6]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Simple Model Building**\n\n### Building a model from scratch using TensorFlow's low-level API.\n\n**Initialization (\\_\\_init\\_\\_ method)**:\n\n*   self.dense1: Represents the first layer with 5 input features and 10 output neurons. It’s initialized with random normal values.\n    \n*   self.dense2: Represents the second layer with 10 input features and 1 output neuron.\n    \n*   **2.Forward Pass (\\_\\_call\\_\\_ method)**:\n    \n    *   **Matrix Multiplication**: tf.matmul(x, self.dense1) computes the dot product between input x and the weights of the first layer.\n        \n    *   **Activation Function**: tf.nn.relu(x) applies the ReLU activation function to introduce non-linearity.\n        \n    *   **Second Matrix Multiplication**: tf.matmul(x, self.dense2) computes the dot product between the output of the activation function and the weights of the second layer.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\nclass SimpleModel(tf.Module):\n    def __init__(self):\n        # Initialize weights for two layers\n        self.dense1 = tf.Variable(tf.random.normal([5, 10]), dtype=tf.float32)  # 5 input features, 10 neurons\n        self.dense2 = tf.Variable(tf.random.normal([10, 1]), dtype=tf.float32)  # 10 neurons, 1 output feature\n\n    def __call__(self, x):\n        # Forward pass through the model\n        x = tf.matmul(x, self.dense1)  # Matrix multiplication\n        x = tf.nn.relu(x)             # ReLU activation\n        x = tf.matmul(x, self.dense2) # Another matrix multiplication\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T12:14:25.394758Z","iopub.execute_input":"2024-09-02T12:14:25.395211Z","iopub.status.idle":"2024-09-02T12:14:25.404940Z","shell.execute_reply.started":"2024-09-02T12:14:25.395171Z","shell.execute_reply":"2024-09-02T12:14:25.403479Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Data preparation involves cleaning, transforming, and organizing data to ensure it’s suitable for training machine learning models. In TensorFlow, the tf.data API is a powerful tool for building efficient and flexible data pipelines, which can include operations like shuffling, batching, and preprocessing to optimize model training and evaluatio","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Create synthetic data\nx_data = np.random.random((100, 5)).astype(np.float32)\ny_data = np.random.random((100, 1)).astype(np.float32)\n\n# Create a TensorFlow dataset\ndataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n\n# Shuffle and batch the dataset\ndataset = dataset.shuffle(buffer_size=100).batch(10)\n\n# Iterate through the dataset\nfor x_batch, y_batch in dataset:\n    print(\"Batch of inputs:\", x_batch.numpy())\n    print(\"Batch of targets:\", y_batch.numpy())","metadata":{"execution":{"iopub.status.busy":"2024-09-02T12:14:25.406983Z","iopub.execute_input":"2024-09-02T12:14:25.407530Z","iopub.status.idle":"2024-09-02T12:14:25.491599Z","shell.execute_reply.started":"2024-09-02T12:14:25.407472Z","shell.execute_reply":"2024-09-02T12:14:25.490091Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Batch of inputs: [[0.5022924  0.6090528  0.77783114 0.11015838 0.0831859 ]\n [0.3691568  0.30995917 0.65048194 0.81051886 0.00461418]\n [0.9505543  0.3746202  0.6093648  0.15278536 0.7194437 ]\n [0.02116243 0.9771841  0.19135043 0.42674702 0.20366372]\n [0.20307198 0.28119823 0.62064385 0.29586577 0.93047124]\n [0.95856524 0.18922615 0.443297   0.7555666  0.6294069 ]\n [0.96670544 0.29472393 0.77239954 0.09888712 0.40259075]\n [0.40480667 0.48696467 0.28061    0.8230132  0.18882361]\n [0.8776611  0.11563282 0.97182274 0.41684422 0.445032  ]\n [0.50174826 0.9802311  0.21975261 0.39084637 0.16310403]]\nBatch of targets: [[0.99463874]\n [0.29450345]\n [0.6661221 ]\n [0.3298658 ]\n [0.55575186]\n [0.48880014]\n [0.20796114]\n [0.61132073]\n [0.09226964]\n [0.5103883 ]]\nBatch of inputs: [[0.1915783  0.70247847 0.41826755 0.0471295  0.5455846 ]\n [0.6965107  0.10033588 0.6624184  0.4906634  0.84845334]\n [0.5164459  0.21045317 0.94720507 0.401826   0.19921386]\n [0.55765486 0.9271016  0.01121936 0.46941766 0.5646055 ]\n [0.5648225  0.8079905  0.97038025 0.91313195 0.6698926 ]\n [0.3103848  0.6813359  0.83443785 0.39959902 0.6537761 ]\n [0.9331392  0.07783116 0.11461254 0.23652354 0.2681927 ]\n [0.6859448  0.68483824 0.9873981  0.3394024  0.11047558]\n [0.9803338  0.33018196 0.03546264 0.23692834 0.07300426]\n [0.6123558  0.70656896 0.3924615  0.25807598 0.25969884]]\nBatch of targets: [[0.73994774]\n [0.7361823 ]\n [0.0372619 ]\n [0.69613594]\n [0.72138834]\n [0.56441516]\n [0.19251186]\n [0.9138041 ]\n [0.01746042]\n [0.70264715]]\nBatch of inputs: [[0.54417    0.56748754 0.40378934 0.25124758 0.95157945]\n [0.43141285 0.2161794  0.7602808  0.5323475  0.24236993]\n [0.23242481 0.1282112  0.22658865 0.963358   0.9520803 ]\n [0.63008845 0.4603708  0.81878537 0.96359795 0.1899215 ]\n [0.78542924 0.61571294 0.8030501  0.5969035  0.70780325]\n [0.3896049  0.11818047 0.1553562  0.6645774  0.57997346]\n [0.46190393 0.08707734 0.51792276 0.42925426 0.11766089]\n [0.08278035 0.63519406 0.43321648 0.5644292  0.05676219]\n [0.09150059 0.62129545 0.96994144 0.16270347 0.29357412]\n [0.33939704 0.8661204  0.83335644 0.89128613 0.7786808 ]]\nBatch of targets: [[0.41309404]\n [0.7266308 ]\n [0.12266643]\n [0.25395122]\n [0.7876364 ]\n [0.03429111]\n [0.08114009]\n [0.8708634 ]\n [0.04627556]\n [0.54978704]]\nBatch of inputs: [[0.11761029 0.6606377  0.435639   0.9128882  0.42353153]\n [0.8790772  0.40178603 0.34963393 0.918985   0.64219964]\n [0.10290167 0.6025051  0.27845335 0.89590853 0.05056727]\n [0.86324567 0.84842587 0.18032591 0.35009634 0.9854698 ]\n [0.75814766 0.34196243 0.0750671  0.21100768 0.8308897 ]\n [0.03244223 0.72317135 0.95208734 0.59690094 0.22891423]\n [0.85361445 0.9075224  0.8878667  0.38081136 0.37422064]\n [0.20467564 0.67557406 0.8647221  0.3246589  0.47637138]\n [0.32647353 0.37370485 0.6190533  0.08099843 0.54390347]\n [0.60071784 0.3141019  0.945197   0.4980811  0.68842   ]]\nBatch of targets: [[0.75462466]\n [0.36640424]\n [0.32843417]\n [0.11535776]\n [0.19871984]\n [0.6910341 ]\n [0.8777823 ]\n [0.43075678]\n [0.7780917 ]\n [0.01808874]]\nBatch of inputs: [[0.7220467  0.9803612  0.57817817 0.7790384  0.23964366]\n [0.7109785  0.45410407 0.7688141  0.3583015  0.17390391]\n [0.5885377  0.4500348  0.5599133  0.5202599  0.8933032 ]\n [0.56561965 0.96556735 0.6437251  0.63629025 0.22436406]\n [0.4925378  0.8375783  0.909835   0.68831027 0.20632319]\n [0.39208895 0.02973296 0.81211907 0.5169664  0.18314853]\n [0.7566888  0.1918     0.08642626 0.68223214 0.6964848 ]\n [0.7376379  0.3159932  0.25831693 0.02376221 0.24547647]\n [0.80041504 0.86175007 0.71890396 0.34401724 0.07616986]\n [0.14507674 0.17284632 0.46439376 0.201492   0.32968995]]\nBatch of targets: [[0.87358475]\n [0.8842037 ]\n [0.38664016]\n [0.9898324 ]\n [0.7926099 ]\n [0.42506036]\n [0.5679707 ]\n [0.75950986]\n [0.39458582]\n [0.14616074]]\nBatch of inputs: [[0.8987624  0.7469966  0.15510629 0.19719678 0.32230148]\n [0.3339908  0.42426732 0.46088636 0.04712869 0.7041014 ]\n [0.5792652  0.5535589  0.40032148 0.77353233 0.44403365]\n [0.14618385 0.2751475  0.08406182 0.47053334 0.5997597 ]\n [0.32290143 0.15326558 0.4357745  0.67631185 0.6682243 ]\n [0.13974103 0.8958337  0.03939281 0.17169142 0.9646794 ]\n [0.26675326 0.22527328 0.2042328  0.37355852 0.9028454 ]\n [0.33200413 0.50539666 0.59012634 0.8666979  0.29098216]\n [0.46103275 0.4836826  0.49624044 0.741072   0.53896165]\n [0.32779387 0.37718156 0.4278105  0.08951806 0.65635234]]\nBatch of targets: [[0.9906909 ]\n [0.1321381 ]\n [0.5386019 ]\n [0.59211653]\n [0.17835066]\n [0.09257124]\n [0.67042106]\n [0.03648679]\n [0.28926128]\n [0.16092408]]\nBatch of inputs: [[0.7086289  0.5559785  0.29300374 0.85196936 0.7532729 ]\n [0.3349778  0.30641925 0.97642606 0.06148987 0.8648108 ]\n [0.18787144 0.20154646 0.17148693 0.9605206  0.21078096]\n [0.1263732  0.33692592 0.72141343 0.75884867 0.3029976 ]\n [0.5404264  0.8650273  0.11903023 0.09216724 0.42818537]\n [0.2707938  0.14890079 0.80229264 0.5909532  0.26729405]\n [0.9117175  0.19799836 0.37127098 0.24846213 0.5927376 ]\n [0.6546413  0.28921357 0.83153313 0.01913648 0.56938505]\n [0.1730152  0.02127752 0.6973208  0.24122873 0.00526886]\n [0.3461256  0.20426346 0.5890288  0.5826968  0.701144  ]]\nBatch of targets: [[0.2527667 ]\n [0.9980011 ]\n [0.02417441]\n [0.97961754]\n [0.7471519 ]\n [0.90743315]\n [0.6119405 ]\n [0.55841696]\n [0.08989078]\n [0.80869234]]\nBatch of inputs: [[0.45363525 0.25975123 0.37883174 0.98561645 0.02563065]\n [0.80109364 0.6020084  0.0681041  0.03203201 0.07088598]\n [0.7815975  0.07057036 0.7622819  0.80011845 0.7081388 ]\n [0.52397066 0.52757174 0.8140629  0.8195563  0.94019926]\n [0.70498633 0.8357094  0.65932107 0.45475355 0.3987587 ]\n [0.97521144 0.4596857  0.41442648 0.53135777 0.7277963 ]\n [0.8621776  0.94272286 0.09908657 0.3276147  0.24539079]\n [0.42134872 0.8721874  0.76605445 0.2826453  0.7115232 ]\n [0.6631509  0.70998347 0.10381737 0.61375564 0.67250407]\n [0.0590096  0.36745444 0.36816987 0.6962835  0.16027942]]\nBatch of targets: [[0.7739758 ]\n [0.2993867 ]\n [0.18408327]\n [0.978686  ]\n [0.9874627 ]\n [0.367332  ]\n [0.3919334 ]\n [0.5668399 ]\n [0.23171054]\n [0.8094372 ]]\nBatch of inputs: [[0.38540643 0.08610117 0.491936   0.58808446 0.6929369 ]\n [0.49386528 0.46890944 0.9466029  0.52863157 0.8543958 ]\n [0.4865806  0.9555812  0.06288428 0.29850495 0.5809504 ]\n [0.47212577 0.89962643 0.5309894  0.47059622 0.7715812 ]\n [0.28026998 0.352651   0.4192173  0.5202477  0.05301906]\n [0.8696418  0.612363   0.35869002 0.31116876 0.0338597 ]\n [0.16295706 0.8801421  0.20222205 0.25219873 0.88197505]\n [0.98580366 0.40663248 0.89441335 0.711688   0.28580117]\n [0.74921525 0.11254737 0.943844   0.4201131  0.5661178 ]\n [0.13386045 0.24765183 0.74016356 0.7775614  0.82576996]]\nBatch of targets: [[0.33210486]\n [0.22859612]\n [0.84108603]\n [0.78780556]\n [0.70361364]\n [0.53046525]\n [0.11261324]\n [0.7730522 ]\n [0.5623878 ]\n [0.6943497 ]]\nBatch of inputs: [[0.74122316 0.85371524 0.3108963  0.194214   0.9172774 ]\n [0.1304166  0.95677    0.07093439 0.8325661  0.78597385]\n [0.17961548 0.5823345  0.58319277 0.42180404 0.3644411 ]\n [0.8816169  0.5521611  0.21087614 0.55308884 0.38819754]\n [0.62637746 0.07358772 0.7687553  0.7458237  0.9490592 ]\n [0.39189458 0.3450169  0.43070343 0.0269757  0.04251053]\n [0.24361186 0.6229763  0.5690947  0.63344973 0.55341697]\n [0.37055826 0.35244638 0.26911664 0.9431103  0.3446432 ]\n [0.14920287 0.2920826  0.06493279 0.12261475 0.9615571 ]\n [0.9250329  0.6183363  0.9223505  0.643162   0.5417068 ]]\nBatch of targets: [[0.22203381]\n [0.321304  ]\n [0.33378327]\n [0.5902073 ]\n [0.59775054]\n [0.18697685]\n [0.5079453 ]\n [0.50990486]\n [0.55216825]\n [0.6144025 ]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Define Loss function and Optimizer\nhere we have implemented a loss function and an optimizer","metadata":{}},{"cell_type":"code","source":"def compute_loss(y_true, y_pred):\n    return tf.reduce_mean(tf.square(y_true - y_pred))  # Mean Squared Error Loss\n\noptimizer = tf.optimizers.Adam()  # Adam optimizer\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T12:14:25.493536Z","iopub.execute_input":"2024-09-02T12:14:25.494083Z","iopub.status.idle":"2024-09-02T12:14:25.595133Z","shell.execute_reply.started":"2024-09-02T12:14:25.494037Z","shell.execute_reply":"2024-09-02T12:14:25.593818Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**Training Loop**\n\nImplement the training loop using TensorFlow’s tf.GradientTape to compute gradients and apply them to the model parameters:","metadata":{}},{"cell_type":"code","source":"# Initialize the model\nmodel = SimpleModel()\n\n# Training loop\nepochs = 5\nfor epoch in range(epochs):\n    epoch_loss = 0\n    for x_batch, y_batch in dataset:\n        with tf.GradientTape() as tape:\n            y_pred = model(x_batch)  # Forward pass\n            loss = compute_loss(y_batch, y_pred)  # Compute loss\n\n        gradients = tape.gradient(loss, model.trainable_variables)  # Compute gradients\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))  # Apply gradients\n\n        epoch_loss += loss.numpy()  # Accumulate loss\n\n    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(dataset)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T12:14:25.596834Z","iopub.execute_input":"2024-09-02T12:14:25.597242Z","iopub.status.idle":"2024-09-02T12:14:26.405154Z","shell.execute_reply.started":"2024-09-02T12:14:25.597202Z","shell.execute_reply":"2024-09-02T12:14:26.403970Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.3416737988591194\nEpoch 2, Loss: 0.3071227766573429\nEpoch 3, Loss: 0.2770118147134781\nEpoch 4, Loss: 0.25374014526605604\nEpoch 5, Loss: 0.23634570837020874\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**5\\. Evaluation**\n\nTo evaluate the model, use the test set (or the same dataset if no separate test set is available). The process is similar to training but without gradient updates:","metadata":{}},{"cell_type":"code","source":"# Evaluation\ndef evaluate_model(model, dataset):\n    total_loss = 0\n    num_batches = 0\n    for x_batch, y_batch in dataset:\n        y_pred = model(x_batch)  # Forward pass\n        loss = compute_loss(y_batch, y_pred)  # Compute loss\n        total_loss += loss.numpy()\n        num_batches += 1\n\n    average_loss = total_loss / num_batches\n    return average_loss\n\n# Evaluate the model on the same dataset (for demonstration purposes)\neval_loss = evaluate_model(model, dataset)\nprint(f\"Evaluation Loss: {eval_loss}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T12:14:26.408975Z","iopub.execute_input":"2024-09-02T12:14:26.409445Z","iopub.status.idle":"2024-09-02T12:14:26.433556Z","shell.execute_reply.started":"2024-09-02T12:14:26.409401Z","shell.execute_reply":"2024-09-02T12:14:26.432383Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Evaluation Loss: 0.2256659597158432\n","output_type":"stream"}]}]}