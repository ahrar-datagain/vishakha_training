{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3036086,"sourceType":"datasetVersion","datasetId":1859421}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('/kaggle/input/housing-prices-dataset/Housing.csv')\nprint(df.columns)\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:06:05.934479Z","iopub.execute_input":"2024-08-27T12:06:05.934948Z","iopub.status.idle":"2024-08-27T12:06:05.947346Z","shell.execute_reply.started":"2024-08-27T12:06:05.934905Z","shell.execute_reply":"2024-08-27T12:06:05.945965Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Index(['price', 'area', 'bedrooms', 'bathrooms', 'stories', 'mainroad',\n       'guestroom', 'basement', 'hotwaterheating', 'airconditioning',\n       'parking', 'prefarea', 'furnishingstatus'],\n      dtype='object')\n(545, 13)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Data Preprocessing\nNow, we categorize the features depending on their datatype (int, float, object) and then calculate the number of them. ","metadata":{}},{"cell_type":"code","source":"obj = (df.dtypes == 'object')\nobject_cols = list(obj[obj].index)\nprint(\"Categorical variables:\",len(object_cols))\n \nint_ = (df.dtypes == 'int')\nnum_cols = list(int_[int_].index)\nprint(\"Integer variables:\",len(num_cols))\n \nfl = (df.dtypes == 'float')\nfl_cols = list(fl[fl].index)\nprint(\"Float variables:\",len(fl_cols))","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:06:53.407174Z","iopub.execute_input":"2024-08-27T12:06:53.407617Z","iopub.status.idle":"2024-08-27T12:06:53.424606Z","shell.execute_reply.started":"2024-08-27T12:06:53.407576Z","shell.execute_reply":"2024-08-27T12:06:53.423098Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Categorical variables: 7\nInteger variables: 6\nFloat variables: 0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"[Data Cleaning](https://www.geeksforgeeks.org/data-preprocessing-in-data-mining/) is the way to improvise the data or remove incorrect, corrupted or irrelevant data.\n\nAs in our dataset, there are some columns that are not important and irrelevant for the model training. So, we can drop that column before training. There are 2 approaches to dealing with empty/null values\n\n*   We can easily delete the column/row (if the feature or record is not much important).\n    \n*   Filling the empty slots with mean/mode/0/NA/etc. (depending on the dataset requirement).","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:18:27.006288Z","iopub.execute_input":"2024-08-27T12:18:27.006868Z","iopub.status.idle":"2024-08-27T12:18:27.136386Z","shell.execute_reply.started":"2024-08-27T12:18:27.006812Z","shell.execute_reply":"2024-08-27T12:18:27.134602Z"}}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:20:51.602803Z","iopub.execute_input":"2024-08-27T12:20:51.603276Z","iopub.status.idle":"2024-08-27T12:20:51.611187Z","shell.execute_reply.started":"2024-08-27T12:20:51.603230Z","shell.execute_reply":"2024-08-27T12:20:51.609672Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"False\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check if there are any missing values in the DataFrame\nhas_missing = df.isnull().values.any()\nprint(has_missing)","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:21:05.022121Z","iopub.execute_input":"2024-08-27T12:21:05.022553Z","iopub.status.idle":"2024-08-27T12:21:05.030126Z","shell.execute_reply.started":"2024-08-27T12:21:05.022510Z","shell.execute_reply":"2024-08-27T12:21:05.028784Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"False\n","output_type":"stream"}]},{"cell_type":"markdown","source":"OneHotEncoder – For Label categorical features\n----------------------------------------------\n\nOne hot Encoding is the best way to convert categorical data into binary vectors. This maps the values to integer values. By using [OneHotEncoder](https://www.geeksforgeeks.org/ml-one-hot-encoding-of-datasets-in-python/), we can easily convert object data into int. So for that, firstly we have to collect all the features which have the object datatype. To do so, we will make a loop.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\ns = (df.dtypes == 'object')\nobject_cols = list(s[s].index)\nprint(\"Categorical variables:\")\nprint(object_cols)\nprint('No. of. categorical features: ', \n\tlen(object_cols))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:22:30.643592Z","iopub.execute_input":"2024-08-27T12:22:30.644021Z","iopub.status.idle":"2024-08-27T12:22:30.652128Z","shell.execute_reply.started":"2024-08-27T12:22:30.643983Z","shell.execute_reply":"2024-08-27T12:22:30.650596Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Categorical variables:\n['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']\nNo. of. categorical features:  7\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Then once we have a list of all the features. We can apply OneHotEncoding to the whole list.","metadata":{}},{"cell_type":"code","source":"OH_encoder = OneHotEncoder(sparse_output=False)\nOH_cols = pd.DataFrame(OH_encoder.fit_transform(df[object_cols]))\nOH_cols.index = df.index\nOH_cols.columns = OH_encoder.get_feature_names_out()\ndf_final = df.drop(object_cols, axis=1)\ndf_final = pd.concat([df_final, OH_cols], axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:24:22.375846Z","iopub.execute_input":"2024-08-27T12:24:22.376316Z","iopub.status.idle":"2024-08-27T12:24:22.392385Z","shell.execute_reply.started":"2024-08-27T12:24:22.376272Z","shell.execute_reply":"2024-08-27T12:24:22.391184Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Splitting Dataset into Training and Testing\n-------------------------------------------\n\nX and Y splitting (i.e. Y is the SalePrice column and the rest of the other columns are X)","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\nX = df_final.drop(['price'], axis=1)\nY = df_final['price']\n\n# Split the training set into \n# training and validation set\nX_train, X_valid, Y_train, Y_valid = train_test_split(\n\tX, Y, train_size=0.8, test_size=0.2, random_state=0)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:25:19.046232Z","iopub.execute_input":"2024-08-27T12:25:19.046674Z","iopub.status.idle":"2024-08-27T12:25:19.172436Z","shell.execute_reply.started":"2024-08-27T12:25:19.046632Z","shell.execute_reply":"2024-08-27T12:25:19.171236Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### **Linear Regression**\n\nLinear Regression predicts the final output-dependent value based on the given independent features. Like, here we have to predict SalePrice depending on features like area, bedrooms, bathrooms, stories, mainroad, guestroom, basement, hotwaterheating, airconditioning, parking, prefarea, furnishingstatus etc.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_percentage_error  # Import the required function\n\n\nmodel_LR = LinearRegression()\nmodel_LR.fit(X_train, Y_train)\nY_pred = model_LR.predict(X_valid)\n\nprint(mean_absolute_percentage_error(Y_valid, Y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:28:21.529145Z","iopub.execute_input":"2024-08-27T12:28:21.529602Z","iopub.status.idle":"2024-08-27T12:28:21.553285Z","shell.execute_reply.started":"2024-08-27T12:28:21.529560Z","shell.execute_reply":"2024-08-27T12:28:21.552135Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"0.16035195155220616\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **SVM – Support vector Machine**\n\nSVM can be used for both regression and classification model. It finds the hyperplane in the n-dimensional plane.","metadata":{}},{"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import mean_absolute_percentage_error\n\nmodel_SVR = svm.SVR()\nmodel_SVR.fit(X_train,Y_train)\nY_pred = model_SVR.predict(X_valid)\n\nprint(mean_absolute_percentage_error(Y_valid, Y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:31:46.500813Z","iopub.execute_input":"2024-08-27T12:31:46.501347Z","iopub.status.idle":"2024-08-27T12:31:46.529306Z","shell.execute_reply.started":"2024-08-27T12:31:46.501304Z","shell.execute_reply":"2024-08-27T12:31:46.528196Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"0.2710074432862681\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Random Forest Regression**\n\nRandom Forest is an ensemble technique that uses multiple of decision trees and can be used for both regression and classification tasks.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nmodel_RFR = RandomForestRegressor(n_estimators=10)\nmodel_RFR.fit(X_train, Y_train)\nY_pred = model_RFR.predict(X_valid)\n\nmean_absolute_percentage_error(Y_valid, Y_pred)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:32:23.207649Z","iopub.execute_input":"2024-08-27T12:32:23.208574Z","iopub.status.idle":"2024-08-27T12:32:23.424202Z","shell.execute_reply.started":"2024-08-27T12:32:23.208526Z","shell.execute_reply":"2024-08-27T12:32:23.422971Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"0.18611592106068287"},"metadata":{}}]},{"cell_type":"markdown","source":"**Note:** Here the Linear Regression predicts outcome with least error.","metadata":{}}]}